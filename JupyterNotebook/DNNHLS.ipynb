{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNNHLS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aS2JxOIoSdU"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vur7AQq_q9Lm"
      },
      "source": [
        "def sigmoid(alpha):\n",
        "    return 1/(1+np.exp(-alpha))\n",
        "def accuracy(scores,ytrain):\n",
        "    predclass = np.argmax(scores,axis=0)\n",
        "    trueclass = np.argmax(ytrain,axis=0)\n",
        "    return (np.sum(predclass==trueclass)/trueclass.size)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9suFDoRt8gAi"
      },
      "source": [
        "def reLU(alpha):\n",
        "  return np.maximum(0,alpha)\n",
        "\n",
        "def reLUprime(alpha):\n",
        "  return np.greater(0,alpha)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nqr149U-upP",
        "outputId": "3ce58448-3549-44d6-e7fe-018f1dd89f33"
      },
      "source": [
        "w_1.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 785)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iVHr2py-xC8",
        "outputId": "efbf90c6-296b-4dc8-b9ce-025c43a14cd0"
      },
      "source": [
        "reLUprime(w_1).shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 785)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpp5rZUxq_Ej",
        "outputId": "b870ed92-c1ff-4b6f-dc91-7a70c9342c2c"
      },
      "source": [
        "#using the previous loadded data set cifar10, reshaped, biasing added\n",
        "#200 neurons,\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "print('x_train: ', x_train.shape)\n",
        "K = len(np.unique(y_train)) # Classes\n",
        "Ntr = x_train.shape[0]\n",
        "Nte = x_test.shape[0]\n",
        "#Din = 3072 # CIFAR10\n",
        "Din = 784 # MINIST\n",
        "# Normalize pixel values\n",
        "#x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "mean_image = np.mean(x_train, axis=0)                   #normalizing\n",
        "x_train = x_train - mean_image\n",
        "x_test = x_test - mean_image\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K).T\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K).T\n",
        "x_train = np.reshape(x_train,(Ntr,Din))\n",
        "x_test = np.reshape(x_test,(Nte,Din))\n",
        "x_train = x_train.astype('float32')\n",
        "x_train = np.insert(x_train,Din,np.zeros((60000,)),axis=1).T # add zeros to include the biasing trick 3073 x 50000\n",
        "x_test = x_test.astype('float32')\n",
        "x_test = np.insert(x_test,Din,np.zeros((10000,)),axis=1).T\n",
        "std=1e-5\n",
        "batchsize = 500\n",
        "H =100                              #hidden nodes (neurons)\n",
        "w_1 = std*np.random.randn(H, Din+1) # biasing trick 200x3073 matrix\n",
        "w_2 = std*np.random.randn(K, H)     # 10x200 matrix\n",
        "epochs = 300    \n",
        "lr = 0.025    \n",
        "lr_decay= 0.975\n",
        "reg =  5e-5\n",
        "loss_history2 = []\n",
        "train_acc_history2 = []\n",
        "val_acc_history2 = []\n",
        "val_loss_history2 = []\n",
        "seed = 0\n",
        "rng = np.random.default_rng(seed=seed)\n",
        "# create random batches per iteration\n",
        "iteration =0\n",
        "epoch =0"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train:  (60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAPwQ8z2rBJ6",
        "outputId": "3e24bcdc-e384-4310-8432-07a5410c78fd"
      },
      "source": [
        "t1 = time.time()\n",
        "while epoch < epochs:\n",
        "    # calculate P1s , send through activation function and get ys, calculate p2s using w_2, calculate loss, get gradient, do gradient descent\n",
        "    indices = np.arange(Ntr)    #shuffling images each epoch\n",
        "    rng.shuffle(indices)\n",
        "    X = x_train[:,indices]\n",
        "    Y_tr = y_train[:,indices]\n",
        "    #calculate P1s\n",
        "    P1 = w_1.dot(X)    #P1 200Xbatchsize\n",
        "    y = sigmoid(P1)    #ys\n",
        "    #y = reLU(P1)\n",
        "    P2 = w_2.dot(y)    #P2, in this case outputs,no activation 10xbatchsize\n",
        "    #explicit implementation of loss\n",
        "    L = (1./Ntr)*((np.square(P2-Y_tr)).sum() + reg*(np.sum(w_1**2)+np.sum(w_2**2)))\n",
        "    #explicit calculation of the two gradients \n",
        "    #PARTIAL DERIVATIVE WRT W2\n",
        "    dL_dP2 = (1./Ntr)*2.0*(P2-Y_tr)\n",
        "    dP2_dw2 = y.T\n",
        "    dL_dw2 = dL_dP2.dot(dP2_dw2) + reg*w_2  #pfffttt \n",
        "    #PARTIAL DERIVATIVE WRT W1\n",
        "    dP2_dy = w_2\n",
        "    dy_dP1 = y*(1-y)\n",
        "    #dy_dP1 = reLUprime(P1)\n",
        "    dP1_dw1= X\n",
        "    dL_dw1 = np.multiply(((dP2_dy.T).dot(dL_dP2)),dy_dP1).dot(dP1_dw1.T)  +reg*w_1    #aaaaaaaaaaaaaaa\n",
        "    w_1 += -lr*dL_dw1\n",
        "    w_2 += -lr*dL_dw2\n",
        "    lr *= lr_decay\n",
        "    loss_history2.append(L)\n",
        "    train_acc_history2.append(accuracy(P2, Y_tr))\n",
        "        \n",
        "    #validation\n",
        "\n",
        "    valP1 = w_1.dot(x_test)    \n",
        "    val_y = sigmoid(valP1)    \n",
        "    #val_y = reLU(valP1)\n",
        "    valP2 = w_2.dot(val_y)\n",
        "    valL = (1./10000)*((np.square(valP2-y_test)).sum() + reg*(np.sum(w_1**2)+np.sum(w_2**2)))\n",
        "    val_loss_history2.append(valL)\n",
        "    val_acc_history2.append(accuracy(valP2, y_test))\n",
        "        \n",
        "    print('epoch:{}, loss: {}, train_acc: {}, validation_loss: {}, validation_acc: {}'.format(epoch,loss_history2[epoch],train_acc_history2[epoch],val_loss_history2[epoch],val_acc_history2[epoch]))\n",
        "    epoch+=1\n",
        "t2 = time.time()\n",
        "print('Execution time : ',t2-t1)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0, loss: 1.0000346422878799, train_acc: 0.09751666666666667, validation_loss: 0.9059410673924382, validation_acc: 0.1135\n",
            "epoch:1, loss: 0.9059746332896375, train_acc: 0.11236666666666667, validation_loss: 0.9008823695169977, validation_acc: 0.1135\n",
            "epoch:2, loss: 0.9008377166954968, train_acc: 0.11236666666666667, validation_loss: 0.9103856886759597, validation_acc: 0.1135\n",
            "epoch:3, loss: 0.9102425384172598, train_acc: 0.11236666666666667, validation_loss: 0.9491746535380365, validation_acc: 0.098\n",
            "epoch:4, loss: 0.948511885446246, train_acc: 0.09871666666666666, validation_loss: 0.9275124209653914, validation_acc: 0.1135\n",
            "epoch:5, loss: 0.9285722743339077, train_acc: 0.11236666666666667, validation_loss: 0.9099117553486739, validation_acc: 0.2782\n",
            "epoch:6, loss: 0.9101711093190599, train_acc: 0.27821666666666667, validation_loss: 0.860637633643195, validation_acc: 0.234\n",
            "epoch:7, loss: 0.8626190844740441, train_acc: 0.23478333333333334, validation_loss: 0.8065567801669219, validation_acc: 0.4452\n",
            "epoch:8, loss: 0.8077649973144247, train_acc: 0.44253333333333333, validation_loss: 0.7692220566246007, validation_acc: 0.5128\n",
            "epoch:9, loss: 0.7706222462269711, train_acc: 0.5091666666666667, validation_loss: 0.7390074204356603, validation_acc: 0.5683\n",
            "epoch:10, loss: 0.7401739653301439, train_acc: 0.56665, validation_loss: 0.7125981036960479, validation_acc: 0.6005\n",
            "epoch:11, loss: 0.7137802284206631, train_acc: 0.5990166666666666, validation_loss: 0.6890190202971318, validation_acc: 0.6266\n",
            "epoch:12, loss: 0.6902201749014877, train_acc: 0.6238, validation_loss: 0.667495339950048, validation_acc: 0.6542\n",
            "epoch:13, loss: 0.6688004912944366, train_acc: 0.64865, validation_loss: 0.6474802643836581, validation_acc: 0.6853\n",
            "epoch:14, loss: 0.6489276481432347, train_acc: 0.67735, validation_loss: 0.6285804691801553, validation_acc: 0.7166\n",
            "epoch:15, loss: 0.6302035033928403, train_acc: 0.7037333333333333, validation_loss: 0.6106358242506412, validation_acc: 0.7405\n",
            "epoch:16, loss: 0.6124358101263075, train_acc: 0.7265833333333334, validation_loss: 0.5935840076227606, validation_acc: 0.7617\n",
            "epoch:17, loss: 0.5955486135792729, train_acc: 0.74735, validation_loss: 0.5774011634647003, validation_acc: 0.7775\n",
            "epoch:18, loss: 0.5795126448722426, train_acc: 0.7644, validation_loss: 0.5620785229225813, validation_acc: 0.7923\n",
            "epoch:19, loss: 0.5643215593363989, train_acc: 0.7806833333333333, validation_loss: 0.5476221396944447, validation_acc: 0.8053\n",
            "epoch:20, loss: 0.5499823105856633, train_acc: 0.7941333333333334, validation_loss: 0.5340328979964042, validation_acc: 0.8162\n",
            "epoch:21, loss: 0.5364970005550155, train_acc: 0.8053666666666667, validation_loss: 0.5212875286292254, validation_acc: 0.8263\n",
            "epoch:22, loss: 0.5238464871209818, train_acc: 0.8144666666666667, validation_loss: 0.5093403242738124, validation_acc: 0.8339\n",
            "epoch:23, loss: 0.5119884135832162, train_acc: 0.8229666666666666, validation_loss: 0.4981366607657412, validation_acc: 0.8404\n",
            "epoch:24, loss: 0.5008684447712296, train_acc: 0.8302333333333334, validation_loss: 0.48762333089195076, validation_acc: 0.8475\n",
            "epoch:25, loss: 0.49043373311898797, train_acc: 0.8366833333333333, validation_loss: 0.47775718836812886, validation_acc: 0.8536\n",
            "epoch:26, loss: 0.4806411693984101, train_acc: 0.8424, validation_loss: 0.46850597578418823, validation_acc: 0.8589\n",
            "epoch:27, loss: 0.47145732762511494, train_acc: 0.8469, validation_loss: 0.4598399587634515, validation_acc: 0.8637\n",
            "epoch:28, loss: 0.46285150854100227, train_acc: 0.8512166666666666, validation_loss: 0.45172555510821805, validation_acc: 0.8664\n",
            "epoch:29, loss: 0.4547901685385799, train_acc: 0.8548166666666667, validation_loss: 0.4441266630392465, validation_acc: 0.8689\n",
            "epoch:30, loss: 0.4472374396885491, train_acc: 0.8581666666666666, validation_loss: 0.4370064943141358, validation_acc: 0.8715\n",
            "epoch:31, loss: 0.4401569310656272, train_acc: 0.861, validation_loss: 0.430329369598549, validation_acc: 0.8739\n",
            "epoch:32, loss: 0.4335139056235583, train_acc: 0.8642833333333333, validation_loss: 0.42406294394089156, validation_acc: 0.8763\n",
            "epoch:33, loss: 0.42727707280360344, train_acc: 0.8665833333333334, validation_loss: 0.41817827224852866, validation_acc: 0.8782\n",
            "epoch:34, loss: 0.4214180986263148, train_acc: 0.86865, validation_loss: 0.4126481578738275, validation_acc: 0.8798\n",
            "epoch:35, loss: 0.41591000682455304, train_acc: 0.8702833333333333, validation_loss: 0.4074463410124044, validation_acc: 0.8823\n",
            "epoch:36, loss: 0.4107265758629781, train_acc: 0.872, validation_loss: 0.4025477567491399, validation_acc: 0.8828\n",
            "epoch:37, loss: 0.40584274409067167, train_acc: 0.8737833333333334, validation_loss: 0.39792896401719996, validation_acc: 0.884\n",
            "epoch:38, loss: 0.4012351461839256, train_acc: 0.8753, validation_loss: 0.3935683894249483, validation_acc: 0.8843\n",
            "epoch:39, loss: 0.39688238332974846, train_acc: 0.87705, validation_loss: 0.38944637490400263, validation_acc: 0.8848\n",
            "epoch:40, loss: 0.3927650366507596, train_acc: 0.8783166666666666, validation_loss: 0.38554509154319616, validation_acc: 0.8858\n",
            "epoch:41, loss: 0.38886554270272, train_acc: 0.8796666666666667, validation_loss: 0.38184839528504344, validation_acc: 0.8863\n",
            "epoch:42, loss: 0.38516802689217855, train_acc: 0.8806833333333334, validation_loss: 0.3783416794865888, validation_acc: 0.8869\n",
            "epoch:43, loss: 0.3816581432371299, train_acc: 0.8814666666666666, validation_loss: 0.3750117342143002, validation_acc: 0.8887\n",
            "epoch:44, loss: 0.3783229323284829, train_acc: 0.88225, validation_loss: 0.37184660836228334, validation_acc: 0.8887\n",
            "epoch:45, loss: 0.3751506945894511, train_acc: 0.8830833333333333, validation_loss: 0.3688354867565274, validation_acc: 0.8895\n",
            "epoch:46, loss: 0.3721308736506729, train_acc: 0.8837833333333334, validation_loss: 0.36596858897716333, validation_acc: 0.8904\n",
            "epoch:47, loss: 0.36925394294967206, train_acc: 0.8845833333333334, validation_loss: 0.3632370706481858, validation_acc: 0.8909\n",
            "epoch:48, loss: 0.36651128752072887, train_acc: 0.8853333333333333, validation_loss: 0.3606329104356035, validation_acc: 0.8917\n",
            "epoch:49, loss: 0.3638950781956747, train_acc: 0.886, validation_loss: 0.35814879254763937, validation_acc: 0.8922\n",
            "epoch:50, loss: 0.3613981450839303, train_acc: 0.8865666666666666, validation_loss: 0.35577798787304427, validation_acc: 0.8925\n",
            "epoch:51, loss: 0.3590138571173916, train_acc: 0.88715, validation_loss: 0.35351422398842547, validation_acc: 0.8934\n",
            "epoch:52, loss: 0.3567360111665715, train_acc: 0.8875333333333333, validation_loss: 0.35135156660262645, validation_acc: 0.8937\n",
            "epoch:53, loss: 0.35455874385053643, train_acc: 0.8881333333333333, validation_loss: 0.34928435697053384, validation_acc: 0.8945\n",
            "epoch:54, loss: 0.35247648473039633, train_acc: 0.8885833333333333, validation_loss: 0.34730721201158027, validation_acc: 0.8953\n",
            "epoch:55, loss: 0.3504839510999408, train_acc: 0.8890833333333333, validation_loss: 0.3454150520950552, validation_acc: 0.896\n",
            "epoch:56, loss: 0.3485761624570396, train_acc: 0.8893666666666666, validation_loss: 0.3436031221268209, validation_acc: 0.8961\n",
            "epoch:57, loss: 0.3467484516148597, train_acc: 0.88975, validation_loss: 0.34186699372471957, validation_acc: 0.8962\n",
            "epoch:58, loss: 0.3449964628997609, train_acc: 0.8902833333333333, validation_loss: 0.3402025517411445, validation_acc: 0.8963\n",
            "epoch:59, loss: 0.343316138952198, train_acc: 0.8907166666666667, validation_loss: 0.33860597240365387, validation_acc: 0.8963\n",
            "epoch:60, loss: 0.341703701099935, train_acc: 0.8911, validation_loss: 0.3370736987784124, validation_acc: 0.897\n",
            "epoch:61, loss: 0.3401556274777835, train_acc: 0.8914166666666666, validation_loss: 0.3356024167722162, validation_acc: 0.8971\n",
            "epoch:62, loss: 0.3386686314044786, train_acc: 0.89185, validation_loss: 0.33418903310327014, validation_acc: 0.8974\n",
            "epoch:63, loss: 0.33723964125312994, train_acc: 0.8922666666666667, validation_loss: 0.3328306556813531, validation_acc: 0.8976\n",
            "epoch:64, loss: 0.3358657822826252, train_acc: 0.8926166666666666, validation_loss: 0.33152457635751564, validation_acc: 0.8976\n",
            "epoch:65, loss: 0.33454436047437786, train_acc: 0.893, validation_loss: 0.3302682557944186, validation_acc: 0.8977\n",
            "epoch:66, loss: 0.3332728482069849, train_acc: 0.8932666666666667, validation_loss: 0.32905931014077244, validation_acc: 0.8981\n",
            "epoch:67, loss: 0.3320488715196017, train_acc: 0.8936, validation_loss: 0.3278954992008775, validation_acc: 0.8981\n",
            "epoch:68, loss: 0.3308701987097071, train_acc: 0.8938, validation_loss: 0.3267747158337103, validation_acc: 0.8984\n",
            "epoch:69, loss: 0.32973473004315645, train_acc: 0.89405, validation_loss: 0.32569497636969297, validation_acc: 0.8987\n",
            "epoch:70, loss: 0.32864048839762516, train_acc: 0.8942, validation_loss: 0.3246544118825081, validation_acc: 0.8988\n",
            "epoch:71, loss: 0.32758561070077186, train_acc: 0.8943333333333333, validation_loss: 0.3236512601924771, validation_acc: 0.899\n",
            "epoch:72, loss: 0.3265683400567906, train_acc: 0.8945666666666666, validation_loss: 0.322683858507314, validation_acc: 0.899\n",
            "epoch:73, loss: 0.3255870184794603, train_acc: 0.8947166666666667, validation_loss: 0.32175063662799813, validation_acc: 0.8992\n",
            "epoch:74, loss: 0.3246400801680951, train_acc: 0.8948333333333334, validation_loss: 0.3208501106645052, validation_acc: 0.8995\n",
            "epoch:75, loss: 0.3237260452766561, train_acc: 0.8951833333333333, validation_loss: 0.3199808772194736, validation_acc: 0.8997\n",
            "epoch:76, loss: 0.3228435141368022, train_acc: 0.8953, validation_loss: 0.319141608007799, validation_acc: 0.8999\n",
            "epoch:77, loss: 0.32199116190333654, train_acc: 0.8954833333333333, validation_loss: 0.3183310448864408, validation_acc: 0.9002\n",
            "epoch:78, loss: 0.321167733595592, train_acc: 0.8956833333333334, validation_loss: 0.31754799527141325, validation_acc: 0.9002\n",
            "epoch:79, loss: 0.32037203951104354, train_acc: 0.89585, validation_loss: 0.3167913279186696, validation_acc: 0.9001\n",
            "epoch:80, loss: 0.3196029509883856, train_acc: 0.896, validation_loss: 0.3160599690435974, validation_acc: 0.9002\n",
            "epoch:81, loss: 0.3188593964971569, train_acc: 0.8961833333333333, validation_loss: 0.3153528987515476, validation_acc: 0.9005\n",
            "epoch:82, loss: 0.31814035803048835, train_acc: 0.8963, validation_loss: 0.3146691477504077, validation_acc: 0.9007\n",
            "epoch:83, loss: 0.31744486777729736, train_acc: 0.8965, validation_loss: 0.3140077943163157, validation_acc: 0.9008\n",
            "epoch:84, loss: 0.3167720050505604, train_acc: 0.8966333333333333, validation_loss: 0.3133679614852343, validation_acc: 0.9009\n",
            "epoch:85, loss: 0.3161208934492241, train_acc: 0.8966666666666666, validation_loss: 0.3127488144458788, validation_acc: 0.9011\n",
            "epoch:86, loss: 0.31549069823271747, train_acc: 0.8968166666666667, validation_loss: 0.3121495581128204, validation_acc: 0.9012\n",
            "epoch:87, loss: 0.31488062388869664, train_acc: 0.8969, validation_loss: 0.31156943486197086, validation_acc: 0.9012\n",
            "epoch:88, loss: 0.3142899118764277, train_acc: 0.8970833333333333, validation_loss: 0.31100772241369534, validation_acc: 0.9014\n",
            "epoch:89, loss: 0.3137178385299249, train_acc: 0.8972833333333333, validation_loss: 0.3104637318513244, validation_acc: 0.9014\n",
            "epoch:90, loss: 0.313163713106591, train_acc: 0.8973666666666666, validation_loss: 0.309936805764799, validation_acc: 0.9015\n",
            "epoch:91, loss: 0.31262687596859945, train_acc: 0.8975333333333333, validation_loss: 0.30942631651064584, validation_acc: 0.9015\n",
            "epoch:92, loss: 0.3121066968856163, train_acc: 0.8975333333333333, validation_loss: 0.3089316645805373, validation_acc: 0.9014\n",
            "epoch:93, loss: 0.3116025734486944, train_acc: 0.8975833333333333, validation_loss: 0.30845227707144873, validation_acc: 0.9016\n",
            "epoch:94, loss: 0.31111392958626566, train_acc: 0.8977166666666667, validation_loss: 0.30798760625098126, validation_acc: 0.9015\n",
            "epoch:95, loss: 0.3106402141741393, train_acc: 0.8978833333333334, validation_loss: 0.30753712821184126, validation_acc: 0.9015\n",
            "epoch:96, loss: 0.3101808997322579, train_acc: 0.89795, validation_loss: 0.30710034160981203, validation_acc: 0.9015\n",
            "epoch:97, loss: 0.3097354812017184, train_acc: 0.89805, validation_loss: 0.3066767664798531, validation_acc: 0.9016\n",
            "epoch:98, loss: 0.3093034747961899, train_acc: 0.8981333333333333, validation_loss: 0.3062659431252398, validation_acc: 0.9016\n",
            "epoch:99, loss: 0.308884416922431, train_acc: 0.8982666666666667, validation_loss: 0.3058674310749266, validation_acc: 0.9017\n",
            "epoch:100, loss: 0.3084778631650773, train_acc: 0.8983833333333333, validation_loss: 0.30548080810458156, validation_acc: 0.9017\n",
            "epoch:101, loss: 0.30808338733129126, train_acc: 0.8984333333333333, validation_loss: 0.30510566931700256, validation_acc: 0.9017\n",
            "epoch:102, loss: 0.30770058055123023, train_acc: 0.8984833333333333, validation_loss: 0.3047416262778872, validation_acc: 0.9019\n",
            "epoch:103, loss: 0.3073290504306116, train_acc: 0.8986333333333333, validation_loss: 0.3043883062031845, validation_acc: 0.902\n",
            "epoch:104, loss: 0.3069684202519418, train_acc: 0.89875, validation_loss: 0.30404535119451054, validation_acc: 0.902\n",
            "epoch:105, loss: 0.3066183282212326, train_acc: 0.8988, validation_loss: 0.30371241751934686, validation_acc: 0.902\n",
            "epoch:106, loss: 0.30627842675726213, train_acc: 0.8989, validation_loss: 0.3033891749329771, validation_acc: 0.9021\n",
            "epoch:107, loss: 0.305948381820653, train_acc: 0.8989, validation_loss: 0.3030753060393396, validation_acc: 0.9021\n",
            "epoch:108, loss: 0.30562787228023236, train_acc: 0.8990166666666667, validation_loss: 0.3027705056881741, validation_acc: 0.9021\n",
            "epoch:109, loss: 0.30531658931431577, train_acc: 0.8990833333333333, validation_loss: 0.30247448040604585, validation_acc: 0.902\n",
            "epoch:110, loss: 0.30501423584472726, train_acc: 0.89915, validation_loss: 0.3021869478590023, validation_acc: 0.9021\n",
            "epoch:111, loss: 0.3047205260015132, train_acc: 0.8992166666666667, validation_loss: 0.30190763634479434, validation_acc: 0.9021\n",
            "epoch:112, loss: 0.30443518461645236, train_acc: 0.8992833333333333, validation_loss: 0.3016362843127437, validation_acc: 0.9022\n",
            "epoch:113, loss: 0.3041579467435916, train_acc: 0.89935, validation_loss: 0.30137263990948376, validation_acc: 0.9023\n",
            "epoch:114, loss: 0.303888557205161, train_acc: 0.8994, validation_loss: 0.30111646054893487, validation_acc: 0.9024\n",
            "epoch:115, loss: 0.303626770161329, train_acc: 0.8994333333333333, validation_loss: 0.3008675125049923, validation_acc: 0.9025\n",
            "epoch:116, loss: 0.3033723487023644, train_acc: 0.8994833333333333, validation_loss: 0.3006255705255197, validation_acc: 0.9025\n",
            "epoch:117, loss: 0.3031250644618654, train_acc: 0.8995166666666666, validation_loss: 0.30039041746634076, validation_acc: 0.9026\n",
            "epoch:118, loss: 0.3028846972498113, train_acc: 0.8995666666666666, validation_loss: 0.3001618439440165, validation_acc: 0.9026\n",
            "epoch:119, loss: 0.3026510347042645, train_acc: 0.8995833333333333, validation_loss: 0.2999396480062798, validation_acc: 0.9026\n",
            "epoch:120, loss: 0.30242387196063725, train_acc: 0.89965, validation_loss: 0.29972363481908, validation_acc: 0.9026\n",
            "epoch:121, loss: 0.30220301133750044, train_acc: 0.8997333333333334, validation_loss: 0.29951361636925944, validation_acc: 0.9026\n",
            "epoch:122, loss: 0.30198826203798634, train_acc: 0.89975, validation_loss: 0.29930941118195326, validation_acc: 0.9027\n",
            "epoch:123, loss: 0.3017794398658889, train_acc: 0.8998, validation_loss: 0.29911084405186295, validation_acc: 0.9029\n",
            "epoch:124, loss: 0.30157636695563, train_acc: 0.8998166666666667, validation_loss: 0.2989177457876129, validation_acc: 0.9028\n",
            "epoch:125, loss: 0.301378871515311, train_acc: 0.89985, validation_loss: 0.298729952968447, validation_acc: 0.9028\n",
            "epoch:126, loss: 0.3011867875821153, train_acc: 0.8998833333333334, validation_loss: 0.29854730771257615, validation_acc: 0.9029\n",
            "epoch:127, loss: 0.30099995478937686, train_acc: 0.8998833333333334, validation_loss: 0.29836965745652655, validation_acc: 0.9029\n",
            "epoch:128, loss: 0.30081821814467197, train_acc: 0.8999, validation_loss: 0.29819685474488383, validation_acc: 0.903\n",
            "epoch:129, loss: 0.3006414278183306, train_acc: 0.89995, validation_loss: 0.2980287570298644, validation_acc: 0.903\n",
            "epoch:130, loss: 0.3004694389417991, train_acc: 0.8999833333333334, validation_loss: 0.2978652264801798, validation_acc: 0.903\n",
            "epoch:131, loss: 0.30030211141532637, train_acc: 0.9000166666666667, validation_loss: 0.2977061297986958, validation_acc: 0.903\n",
            "epoch:132, loss: 0.30013930972447, train_acc: 0.9001, validation_loss: 0.29755133804841605, validation_acc: 0.903\n",
            "epoch:133, loss: 0.29998090276495337, train_acc: 0.9001333333333333, validation_loss: 0.29740072648634813, validation_acc: 0.903\n",
            "epoch:134, loss: 0.29982676367543865, train_acc: 0.9001833333333333, validation_loss: 0.2972541744048393, validation_acc: 0.903\n",
            "epoch:135, loss: 0.29967676967778917, train_acc: 0.9002166666666667, validation_loss: 0.29711156497999214, validation_acc: 0.903\n",
            "epoch:136, loss: 0.2995308019244424, train_acc: 0.90025, validation_loss: 0.29697278512679076, validation_acc: 0.903\n",
            "epoch:137, loss: 0.2993887453525193, train_acc: 0.90025, validation_loss: 0.296837725360595, validation_acc: 0.9031\n",
            "epoch:138, loss: 0.29925048854432446, train_acc: 0.9002166666666667, validation_loss: 0.2967062796646773, validation_acc: 0.9031\n",
            "epoch:139, loss: 0.29911592359391376, train_acc: 0.9002833333333333, validation_loss: 0.2965783453634944, validation_acc: 0.9031\n",
            "epoch:140, loss: 0.29898494597941955, train_acc: 0.9004, validation_loss: 0.296453823001407, validation_acc: 0.9031\n",
            "epoch:141, loss: 0.2988574544408473, train_acc: 0.9004, validation_loss: 0.29633261622657264, validation_acc: 0.9032\n",
            "epoch:142, loss: 0.29873335086306463, train_acc: 0.9004833333333333, validation_loss: 0.29621463167975787, validation_acc: 0.9034\n",
            "epoch:143, loss: 0.298612540163732, train_acc: 0.9005166666666666, validation_loss: 0.29609977888782596, validation_acc: 0.9035\n",
            "epoch:144, loss: 0.2984949301859265, train_acc: 0.9004833333333333, validation_loss: 0.2959879701616701, validation_acc: 0.9035\n",
            "epoch:145, loss: 0.2983804315952335, train_acc: 0.9005166666666666, validation_loss: 0.2958791204983784, validation_acc: 0.9035\n",
            "epoch:146, loss: 0.2982689577810846, train_acc: 0.9005666666666666, validation_loss: 0.29577314748742406, validation_acc: 0.9035\n",
            "epoch:147, loss: 0.29816042476214116, train_acc: 0.9005666666666666, validation_loss: 0.29566997122069, validation_acc: 0.9035\n",
            "epoch:148, loss: 0.2980547510955274, train_acc: 0.9006, validation_loss: 0.29556951420614364, validation_acc: 0.9035\n",
            "epoch:149, loss: 0.2979518577897321, train_acc: 0.9006166666666666, validation_loss: 0.2954717012849891, validation_acc: 0.9035\n",
            "epoch:150, loss: 0.2978516682210017, train_acc: 0.9006166666666666, validation_loss: 0.2953764595521356, validation_acc: 0.9036\n",
            "epoch:151, loss: 0.29775410805306646, train_acc: 0.9006166666666666, validation_loss: 0.29528371827982464, validation_acc: 0.9036\n",
            "epoch:152, loss: 0.2976591051600405, train_acc: 0.9006166666666666, validation_loss: 0.29519340884427214, validation_acc: 0.9037\n",
            "epoch:153, loss: 0.2975665895523494, train_acc: 0.9006333333333333, validation_loss: 0.2951054646551844, validation_acc: 0.9037\n",
            "epoch:154, loss: 0.29747649330554876, train_acc: 0.90065, validation_loss: 0.2950198210880182, validation_acc: 0.9037\n",
            "epoch:155, loss: 0.2973887504918994, train_acc: 0.9006666666666666, validation_loss: 0.2949364154188606, validation_acc: 0.9037\n",
            "epoch:156, loss: 0.29730329711457437, train_acc: 0.9006666666666666, validation_loss: 0.29485518676180844, validation_acc: 0.9038\n",
            "epoch:157, loss: 0.2972200710443819, train_acc: 0.9007, validation_loss: 0.29477607600873773, validation_acc: 0.9039\n",
            "epoch:158, loss: 0.2971390119588887, train_acc: 0.9007166666666667, validation_loss: 0.2946990257713555, validation_acc: 0.904\n",
            "epoch:159, loss: 0.2970600612838389, train_acc: 0.9007166666666667, validation_loss: 0.29462398032543297, validation_acc: 0.904\n",
            "epoch:160, loss: 0.2969831621367687, train_acc: 0.9007333333333334, validation_loss: 0.2945508855571258, validation_acc: 0.904\n",
            "epoch:161, loss: 0.29690825927271625, train_acc: 0.9007666666666667, validation_loss: 0.29447968891128834, validation_acc: 0.904\n",
            "epoch:162, loss: 0.2968352990319417, train_acc: 0.9007666666666667, validation_loss: 0.29441033934169686, validation_acc: 0.904\n",
            "epoch:163, loss: 0.296764229289565, train_acc: 0.9007666666666667, validation_loss: 0.2943427872630995, validation_acc: 0.9041\n",
            "epoch:164, loss: 0.2966949994070434, train_acc: 0.90075, validation_loss: 0.2942769845050148, validation_acc: 0.9041\n",
            "epoch:165, loss: 0.29662756018540803, train_acc: 0.9007666666666667, validation_loss: 0.2942128842672041, validation_acc: 0.9041\n",
            "epoch:166, loss: 0.2965618638201859, train_acc: 0.9007666666666667, validation_loss: 0.2941504410767479, validation_acc: 0.9041\n",
            "epoch:167, loss: 0.2964978638579364, train_acc: 0.9008166666666667, validation_loss: 0.29408961074665835, validation_acc: 0.9041\n",
            "epoch:168, loss: 0.2964355151543349, train_acc: 0.9008166666666667, validation_loss: 0.2940303503359642, validation_acc: 0.9041\n",
            "epoch:169, loss: 0.2963747738337397, train_acc: 0.9008333333333334, validation_loss: 0.2939726181112082, validation_acc: 0.9041\n",
            "epoch:170, loss: 0.2963155972501805, train_acc: 0.9008333333333334, validation_loss: 0.29391637350929656, validation_acc: 0.9041\n",
            "epoch:171, loss: 0.29625794394971083, train_acc: 0.9008333333333334, validation_loss: 0.2938615771016483, validation_acc: 0.9042\n",
            "epoch:172, loss: 0.29620177363406824, train_acc: 0.9008666666666667, validation_loss: 0.2938081905595892, validation_acc: 0.9042\n",
            "epoch:173, loss: 0.29614704712559237, train_acc: 0.9008666666666667, validation_loss: 0.29375617662094183, validation_acc: 0.9042\n",
            "epoch:174, loss: 0.29609372633334424, train_acc: 0.9008833333333334, validation_loss: 0.29370549905776305, validation_acc: 0.9042\n",
            "epoch:175, loss: 0.29604177422038686, train_acc: 0.9009, validation_loss: 0.2936561226451838, validation_acc: 0.9043\n",
            "epoch:176, loss: 0.29599115477217436, train_acc: 0.9009166666666667, validation_loss: 0.29360801313130613, validation_acc: 0.9043\n",
            "epoch:177, loss: 0.29594183296600973, train_acc: 0.9009166666666667, validation_loss: 0.29356113720811944, validation_acc: 0.9043\n",
            "epoch:178, loss: 0.29589377474152934, train_acc: 0.9009166666666667, validation_loss: 0.2935154624833909, validation_acc: 0.9043\n",
            "epoch:179, loss: 0.29584694697217145, train_acc: 0.9009, validation_loss: 0.29347095745349805, validation_acc: 0.9043\n",
            "epoch:180, loss: 0.29580131743759597, train_acc: 0.9009, validation_loss: 0.29342759147716213, validation_acc: 0.9044\n",
            "epoch:181, loss: 0.2957568547970122, train_acc: 0.9009, validation_loss: 0.29338533475005196, validation_acc: 0.9044\n",
            "epoch:182, loss: 0.29571352856338623, train_acc: 0.9009166666666667, validation_loss: 0.29334415828022115, validation_acc: 0.9044\n",
            "epoch:183, loss: 0.29567130907849193, train_acc: 0.9009166666666667, validation_loss: 0.2933040338643511, validation_acc: 0.9044\n",
            "epoch:184, loss: 0.2956301674887724, train_acc: 0.9009333333333334, validation_loss: 0.2932649340647663, validation_acc: 0.9044\n",
            "epoch:185, loss: 0.2955900757219845, train_acc: 0.9009333333333334, validation_loss: 0.2932268321871944, validation_acc: 0.9044\n",
            "epoch:186, loss: 0.2955510064645955, train_acc: 0.90095, validation_loss: 0.29318970225924473, validation_acc: 0.9044\n",
            "epoch:187, loss: 0.29551293313990457, train_acc: 0.90095, validation_loss: 0.2931535190095749, validation_acc: 0.9044\n",
            "epoch:188, loss: 0.29547582988686266, train_acc: 0.9009333333333334, validation_loss: 0.2931182578477252, validation_acc: 0.9044\n",
            "epoch:189, loss: 0.2954396715395644, train_acc: 0.9009333333333334, validation_loss: 0.29308389484459185, validation_acc: 0.9044\n",
            "epoch:190, loss: 0.2954044336073896, train_acc: 0.9009166666666667, validation_loss: 0.2930504067135193, validation_acc: 0.9044\n",
            "epoch:191, loss: 0.29537009225576843, train_acc: 0.9009333333333334, validation_loss: 0.29301777079198726, validation_acc: 0.9044\n",
            "epoch:192, loss: 0.2953366242875492, train_acc: 0.9009333333333334, validation_loss: 0.2929859650238718, validation_acc: 0.9044\n",
            "epoch:193, loss: 0.2953040071249486, train_acc: 0.9009333333333334, validation_loss: 0.2929549679422606, validation_acc: 0.9045\n",
            "epoch:194, loss: 0.2952722187920604, train_acc: 0.9009333333333334, validation_loss: 0.2929247586528011, validation_acc: 0.9045\n",
            "epoch:195, loss: 0.29524123789790585, train_acc: 0.90095, validation_loss: 0.2928953168175648, validation_acc: 0.9045\n",
            "epoch:196, loss: 0.2952110436200065, train_acc: 0.90095, validation_loss: 0.2928666226394086, validation_acc: 0.9045\n",
            "epoch:197, loss: 0.2951816156884587, train_acc: 0.90095, validation_loss: 0.29283865684681487, validation_acc: 0.9045\n",
            "epoch:198, loss: 0.29515293437049617, train_acc: 0.9009666666666667, validation_loss: 0.2928114006791955, validation_acc: 0.9045\n",
            "epoch:199, loss: 0.29512498045552066, train_acc: 0.9009666666666667, validation_loss: 0.29278483587264326, validation_acc: 0.9046\n",
            "epoch:200, loss: 0.29509773524058586, train_acc: 0.9009666666666667, validation_loss: 0.2927589446461145, validation_acc: 0.9046\n",
            "epoch:201, loss: 0.2950711805163207, train_acc: 0.9009666666666667, validation_loss: 0.29273370968802886, validation_acc: 0.9046\n",
            "epoch:202, loss: 0.2950452985532726, train_acc: 0.9009666666666667, validation_loss: 0.2927091141432723, validation_acc: 0.9046\n",
            "epoch:203, loss: 0.2950200720886633, train_acc: 0.90095, validation_loss: 0.29268514160058934, validation_acc: 0.9046\n",
            "epoch:204, loss: 0.29499548431353567, train_acc: 0.9009333333333334, validation_loss: 0.29266177608035115, validation_acc: 0.9046\n",
            "epoch:205, loss: 0.29497151886028516, train_acc: 0.90095, validation_loss: 0.29263900202268817, validation_acc: 0.9046\n",
            "epoch:206, loss: 0.2949481597905594, train_acc: 0.9009333333333334, validation_loss: 0.2926168042759736, validation_acc: 0.9046\n",
            "epoch:207, loss: 0.2949253915835143, train_acc: 0.9009333333333334, validation_loss: 0.29259516808564723, validation_acc: 0.9046\n",
            "epoch:208, loss: 0.29490319912441615, train_acc: 0.9009333333333334, validation_loss: 0.29257407908336763, validation_acc: 0.9046\n",
            "epoch:209, loss: 0.2948815676935767, train_acc: 0.9009333333333334, validation_loss: 0.29255352327648304, validation_acc: 0.9046\n",
            "epoch:210, loss: 0.29486048295561196, train_acc: 0.9009666666666667, validation_loss: 0.2925334870378092, validation_acc: 0.9046\n",
            "epoch:211, loss: 0.2948399309490129, train_acc: 0.9009666666666667, validation_loss: 0.29251395709570555, validation_acc: 0.9046\n",
            "epoch:212, loss: 0.2948198980760201, train_acc: 0.9009833333333334, validation_loss: 0.2924949205244387, validation_acc: 0.9046\n",
            "epoch:213, loss: 0.294800371092788, train_acc: 0.901, validation_loss: 0.29247636473482574, validation_acc: 0.9046\n",
            "epoch:214, loss: 0.2947813370998365, train_acc: 0.901, validation_loss: 0.2924582774651464, validation_acc: 0.9046\n",
            "epoch:215, loss: 0.2947627835327739, train_acc: 0.901, validation_loss: 0.29244064677231685, validation_acc: 0.9046\n",
            "epoch:216, loss: 0.2947446981532872, train_acc: 0.901, validation_loss: 0.29242346102331707, validation_acc: 0.9046\n",
            "epoch:217, loss: 0.29472706904038826, train_acc: 0.901, validation_loss: 0.2924067088868626, validation_acc: 0.9046\n",
            "epoch:218, loss: 0.2947098845819113, train_acc: 0.9010166666666667, validation_loss: 0.29239037932531414, validation_acc: 0.9046\n",
            "epoch:219, loss: 0.2946931334662496, train_acc: 0.9010166666666667, validation_loss: 0.29237446158681635, validation_acc: 0.9046\n",
            "epoch:220, loss: 0.2946768046743274, train_acc: 0.9010333333333334, validation_loss: 0.29235894519766004, validation_acc: 0.9046\n",
            "epoch:221, loss: 0.2946608874717988, train_acc: 0.9010333333333334, validation_loss: 0.29234381995486053, validation_acc: 0.9046\n",
            "epoch:222, loss: 0.29464537140146546, train_acc: 0.9010666666666667, validation_loss: 0.2923290759189439, validation_acc: 0.9046\n",
            "epoch:223, loss: 0.2946302462759068, train_acc: 0.9010666666666667, validation_loss: 0.2923147034069372, validation_acc: 0.9046\n",
            "epoch:224, loss: 0.2946155021703192, train_acc: 0.9010666666666667, validation_loss: 0.29230069298555517, validation_acc: 0.9046\n",
            "epoch:225, loss: 0.2946011294155519, train_acc: 0.9010666666666667, validation_loss: 0.29228703546457613, validation_acc: 0.9046\n",
            "epoch:226, loss: 0.2945871185913398, train_acc: 0.9010666666666667, validation_loss: 0.29227372189040474, validation_acc: 0.9046\n",
            "epoch:227, loss: 0.2945734605197223, train_acc: 0.9010666666666667, validation_loss: 0.29226074353981213, validation_acc: 0.9046\n",
            "epoch:228, loss: 0.29456014625864674, train_acc: 0.9010666666666667, validation_loss: 0.2922480919138508, validation_acc: 0.9046\n",
            "epoch:229, loss: 0.2945471670957472, train_acc: 0.9010833333333333, validation_loss: 0.29223575873193747, validation_acc: 0.9046\n",
            "epoch:230, loss: 0.29453451454229523, train_acc: 0.9010666666666667, validation_loss: 0.29222373592609985, validation_acc: 0.9046\n",
            "epoch:231, loss: 0.2945221803273188, train_acc: 0.9010666666666667, validation_loss: 0.29221201563538196, validation_acc: 0.9046\n",
            "epoch:232, loss: 0.2945101563918813, train_acc: 0.9010666666666667, validation_loss: 0.29220059020040295, validation_acc: 0.9046\n",
            "epoch:233, loss: 0.29449843488351685, train_acc: 0.9010666666666667, validation_loss: 0.2921894521580659, validation_acc: 0.9045\n",
            "epoch:234, loss: 0.2944870081508208, train_acc: 0.9010833333333333, validation_loss: 0.29217859423641124, validation_acc: 0.9045\n",
            "epoch:235, loss: 0.29447586873818515, train_acc: 0.9010833333333333, validation_loss: 0.29216800934961007, validation_acc: 0.9045\n",
            "epoch:236, loss: 0.2944650093806784, train_acc: 0.9010833333333333, validation_loss: 0.29215769059309626, validation_acc: 0.9045\n",
            "epoch:237, loss: 0.29445442299906605, train_acc: 0.9010833333333333, validation_loss: 0.2921476312388282, validation_acc: 0.9045\n",
            "epoch:238, loss: 0.29444410269496285, train_acc: 0.9010833333333333, validation_loss: 0.29213782473068167, validation_acc: 0.9045\n",
            "epoch:239, loss: 0.2944340417461204, train_acc: 0.9010833333333333, validation_loss: 0.2921282646799661, validation_acc: 0.9045\n",
            "epoch:240, loss: 0.2944242336018408, train_acc: 0.9010833333333333, validation_loss: 0.29211894486106316, validation_acc: 0.9045\n",
            "epoch:241, loss: 0.2944146718785114, train_acc: 0.9010833333333333, validation_loss: 0.2921098592071828, validation_acc: 0.9045\n",
            "epoch:242, loss: 0.29440535035526527, train_acc: 0.9010833333333333, validation_loss: 0.29210100180623355, validation_acc: 0.9045\n",
            "epoch:243, loss: 0.29439626296975363, train_acc: 0.9010833333333333, validation_loss: 0.2920923668968044, validation_acc: 0.9045\n",
            "epoch:244, loss: 0.29438740381403333, train_acc: 0.9010833333333333, validation_loss: 0.29208394886425454, validation_acc: 0.9045\n",
            "epoch:245, loss: 0.2943787671305664, train_acc: 0.9011, validation_loss: 0.2920757422369078, validation_acc: 0.9045\n",
            "epoch:246, loss: 0.2943703473083244, train_acc: 0.9011, validation_loss: 0.29206774168234945, validation_acc: 0.9045\n",
            "epoch:247, loss: 0.2943621388789977, train_acc: 0.9011, validation_loss: 0.29205994200382157, validation_acc: 0.9045\n",
            "epoch:248, loss: 0.2943541365133058, train_acc: 0.9011166666666667, validation_loss: 0.2920523381367149, validation_acc: 0.9045\n",
            "epoch:249, loss: 0.29434633501740654, train_acc: 0.9011166666666667, validation_loss: 0.2920449251451547, validation_acc: 0.9045\n",
            "epoch:250, loss: 0.2943387293294001, train_acc: 0.9011333333333333, validation_loss: 0.29203769821867603, validation_acc: 0.9045\n",
            "epoch:251, loss: 0.29433131451592637, train_acc: 0.9011333333333333, validation_loss: 0.2920306526689896, validation_acc: 0.9045\n",
            "epoch:252, loss: 0.2943240857688521, train_acc: 0.9011333333333333, validation_loss: 0.29202378392683115, validation_acc: 0.9045\n",
            "epoch:253, loss: 0.2943170384020466, train_acc: 0.90115, validation_loss: 0.2920170875388966, validation_acc: 0.9045\n",
            "epoch:254, loss: 0.29431016784824177, train_acc: 0.90115, validation_loss: 0.2920105591648558, validation_acc: 0.9045\n",
            "epoch:255, loss: 0.294303469655976, train_acc: 0.90115, validation_loss: 0.2920041945744478, validation_acc: 0.9045\n",
            "epoch:256, loss: 0.29429693948661767, train_acc: 0.90115, validation_loss: 0.29199798964465135, validation_acc: 0.9045\n",
            "epoch:257, loss: 0.29429057311146833, train_acc: 0.90115, validation_loss: 0.29199194035692916, validation_acc: 0.9045\n",
            "epoch:258, loss: 0.2942843664089413, train_acc: 0.90115, validation_loss: 0.2919860427945477, validation_acc: 0.9045\n",
            "epoch:259, loss: 0.29427831536181465, train_acc: 0.90115, validation_loss: 0.291980293139964, validation_acc: 0.9045\n",
            "epoch:260, loss: 0.29427241605455595, train_acc: 0.90115, validation_loss: 0.2919746876722833, validation_acc: 0.9045\n",
            "epoch:261, loss: 0.2942666646707181, train_acc: 0.90115, validation_loss: 0.2919692227647831, validation_acc: 0.9045\n",
            "epoch:262, loss: 0.2942610574904017, train_acc: 0.9011666666666667, validation_loss: 0.2919638948825012, validation_acc: 0.9045\n",
            "epoch:263, loss: 0.29425559088778697, train_acc: 0.9011666666666667, validation_loss: 0.29195870057988776, validation_acc: 0.9045\n",
            "epoch:264, loss: 0.2942502613287256, train_acc: 0.9011666666666667, validation_loss: 0.29195363649851847, validation_acc: 0.9045\n",
            "epoch:265, loss: 0.29424506536840006, train_acc: 0.9011666666666667, validation_loss: 0.2919486993648675, validation_acc: 0.9045\n",
            "epoch:266, loss: 0.2942399996490401, train_acc: 0.9011666666666667, validation_loss: 0.2919438859881385, validation_acc: 0.9045\n",
            "epoch:267, loss: 0.29423506089770257, train_acc: 0.9011666666666667, validation_loss: 0.29193919325815193, validation_acc: 0.9045\n",
            "epoch:268, loss: 0.2942302459241046, train_acc: 0.9011666666666667, validation_loss: 0.29193461814328786, validation_acc: 0.9045\n",
            "epoch:269, loss: 0.2942255516185182, train_acc: 0.9011666666666667, validation_loss: 0.29193015768848224, validation_acc: 0.9045\n",
            "epoch:270, loss: 0.2942209749497149, train_acc: 0.9011666666666667, validation_loss: 0.29192580901327425, validation_acc: 0.9045\n",
            "epoch:271, loss: 0.29421651296296597, train_acc: 0.9011666666666667, validation_loss: 0.29192156930990626, validation_acc: 0.9045\n",
            "epoch:272, loss: 0.2942121627780948, train_acc: 0.9011666666666667, validation_loss: 0.2919174358414712, validation_acc: 0.9045\n",
            "epoch:273, loss: 0.29420792158757975, train_acc: 0.9011666666666667, validation_loss: 0.29191340594010906, validation_acc: 0.9045\n",
            "epoch:274, loss: 0.2942037866547036, train_acc: 0.9011666666666667, validation_loss: 0.2919094770052496, validation_acc: 0.9045\n",
            "epoch:275, loss: 0.29419975531175524, train_acc: 0.9011666666666667, validation_loss: 0.291905646501901, validation_acc: 0.9045\n",
            "epoch:276, loss: 0.29419582495827307, train_acc: 0.9011666666666667, validation_loss: 0.2919019119589819, validation_acc: 0.9045\n",
            "epoch:277, loss: 0.29419199305933874, train_acc: 0.9011833333333333, validation_loss: 0.2918982709676977, validation_acc: 0.9045\n",
            "epoch:278, loss: 0.2941882571439095, train_acc: 0.9011833333333333, validation_loss: 0.29189472117995785, validation_acc: 0.9045\n",
            "epoch:279, loss: 0.29418461480319824, train_acc: 0.9012, validation_loss: 0.2918912603068342, validation_acc: 0.9045\n",
            "epoch:280, loss: 0.29418106368909275, train_acc: 0.9012, validation_loss: 0.2918878861170592, validation_acc: 0.9045\n",
            "epoch:281, loss: 0.29417760151261646, train_acc: 0.9012, validation_loss: 0.29188459643556275, validation_acc: 0.9045\n",
            "epoch:282, loss: 0.2941742260424282, train_acc: 0.9012, validation_loss: 0.2918813891420466, validation_acc: 0.9045\n",
            "epoch:283, loss: 0.2941709351033614, train_acc: 0.9012, validation_loss: 0.2918782621695958, validation_acc: 0.9045\n",
            "epoch:284, loss: 0.2941677265750003, train_acc: 0.9012, validation_loss: 0.2918752135033249, validation_acc: 0.9045\n",
            "epoch:285, loss: 0.29416459839029235, train_acc: 0.9012, validation_loss: 0.29187224117906024, validation_acc: 0.9045\n",
            "epoch:286, loss: 0.2941615485341974, train_acc: 0.9012, validation_loss: 0.2918693432820546, validation_acc: 0.9045\n",
            "epoch:287, loss: 0.29415857504237, train_acc: 0.9012, validation_loss: 0.29186651794573626, validation_acc: 0.9045\n",
            "epoch:288, loss: 0.29415567599987685, train_acc: 0.9012, validation_loss: 0.2918637633504891, validation_acc: 0.9045\n",
            "epoch:289, loss: 0.29415284953994597, train_acc: 0.9012, validation_loss: 0.2918610777224644, validation_acc: 0.9045\n",
            "epoch:290, loss: 0.29415009384274854, train_acc: 0.9012, validation_loss: 0.29185845933242266, validation_acc: 0.9045\n",
            "epoch:291, loss: 0.29414740713421217, train_acc: 0.9012, validation_loss: 0.29185590649460597, validation_acc: 0.9045\n",
            "epoch:292, loss: 0.29414478768486363, train_acc: 0.9012, validation_loss: 0.2918534175656376, validation_acc: 0.9045\n",
            "epoch:293, loss: 0.2941422338087019, train_acc: 0.9012, validation_loss: 0.29185099094345174, validation_acc: 0.9045\n",
            "epoch:294, loss: 0.29413974386209896, train_acc: 0.9012, validation_loss: 0.2918486250662488, validation_acc: 0.9045\n",
            "epoch:295, loss: 0.2941373162427312, train_acc: 0.9012, validation_loss: 0.2918463184114788, validation_acc: 0.9045\n",
            "epoch:296, loss: 0.2941349493885339, train_acc: 0.9012, validation_loss: 0.2918440694948492, validation_acc: 0.9045\n",
            "epoch:297, loss: 0.2941326417766869, train_acc: 0.9012, validation_loss: 0.2918418768693594, validation_acc: 0.9045\n",
            "epoch:298, loss: 0.2941303919226226, train_acc: 0.9012, validation_loss: 0.29183973912435934, validation_acc: 0.9045\n",
            "epoch:299, loss: 0.29412819837906107, train_acc: 0.9012, validation_loss: 0.29183765488463176, validation_acc: 0.9045\n",
            "Execution time :  276.8041958808899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE0u1P5BImDz",
        "outputId": "3dc52e3a-c811-496d-af30-3980c701e61a"
      },
      "source": [
        "np.amax(w_2)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03966799521878954"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDrlsNcRwm9T",
        "outputId": "bf965a63-e26f-4dda-a4e9-7b52f55409ee"
      },
      "source": [
        "w_1.T.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(785, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM5JRg6sw2Rw",
        "outputId": "c616c5f4-e069-4dd4-d0df-53078cf94ac7"
      },
      "source": [
        "w_2.T.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "polqAfP4w_9T"
      },
      "source": [
        "height = 100\n",
        "width =10\n",
        "with open(\"W2.txt\" ,\"w+\" )as file:\n",
        "    W1 = w_2.T\n",
        "    file.write(\"{\")\n",
        "    for i in range(height):\n",
        "        file.write(\"{\")\n",
        "        for j in range(width):\n",
        "            if j==width-1:\n",
        "                file.write(str(W1[i][j]))\n",
        "            else:\n",
        "                file.write(str(W1[i][j]))\n",
        "                file.write(\",\")\n",
        "        file.write(\"},\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3racTAAzE3k",
        "outputId": "348af3d4-1258-4421-8828-e5d4ce0dd851"
      },
      "source": [
        "x_test[:,0].shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(785,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZCl58N1d1Z"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "Ntr = x_train.shape[0]\n",
        "Nte = x_test.shape[0]\n",
        "x_test = np.reshape(x_test,(Nte,Din))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gny4hZz1jKZ"
      },
      "source": [
        "x_test = np.insert(x_test,Din,np.zeros((10000,)),axis=1).T"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF1Hy5Ndz2fU"
      },
      "source": [
        "temp = x_test[:,1]\n",
        "with open(\"image.txt\",\"w+\") as file:\n",
        "  file.write(\"{\")\n",
        "  for i in range(785):\n",
        "    if i ==784:\n",
        "      file.write(str(temp[i]))\n",
        "      file.write(\"}\")\n",
        "    else:  \n",
        "      file.write(str(temp[i]))\n",
        "      file.write(\",\")\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdeZcfFCTAMl"
      },
      "source": [
        "temp = np.delete(temp, 784)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqZLXzwdSvO3"
      },
      "source": [
        "\n",
        "image = np.reshape(temp,(28,28))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "1mO6cm2jUHq9",
        "outputId": "2469833a-02d8-4d82-d8f9-b093c9723e23"
      },
      "source": [
        "plt.imshow(image,'gray')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbf95a50fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qTFYJdizHVA"
      },
      "source": [
        "P1 = w_1.dot(x_test[:,1])    #P1 200Xbatchsize\n",
        "y = sigmoid(P1)    #ys\n",
        "P2 = w_2.dot(y)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xx7diYV0xHQ",
        "outputId": "5ae10325-9606-4d5e-b542-913fccfc04c2"
      },
      "source": [
        "print(P2)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.19013278 -0.10137853  0.21697459  0.28671252 -0.12292113  0.19689055\n",
            "  0.14057958 -0.06545267  0.1646671  -0.106307  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YJF3n7oWL1F",
        "outputId": "b5214a84-5bf6-4f4d-cfe3-e00e822f0b89"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "(x_train, y_train),(x_test,y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\")/255.0   # normalize\n",
        "x_test = x_test.astype(\"float32\")/255.0     # normalize\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0],True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SxXC7FPWO3_",
        "outputId": "17b788e8-41a8-430a-c9f3-f1ddb037365b"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(32, 32, 3)),\n",
        "        layers.Conv2D(32, 3, padding=\"valid\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(64, 3, activation=\"relu\"),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Conv2D(16, 3, activation=\"relu\"),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "print(model.summary())\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=0.002,momentum=0.0001, name='SGD'),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=50, epochs=200, verbose=2,validation_data=(x_test,y_test))\n",
        "#model.evaluate(x_test, y_test, batch_size=50, verbose=2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 4, 4, 16)          9232      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 33,434\n",
            "Trainable params: 33,434\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "1000/1000 - 4s - loss: 2.2914 - accuracy: 0.1392 - val_loss: 2.2796 - val_accuracy: 0.1741\n",
            "Epoch 2/200\n",
            "1000/1000 - 3s - loss: 2.2571 - accuracy: 0.1866 - val_loss: 2.2221 - val_accuracy: 0.1967\n",
            "Epoch 3/200\n",
            "1000/1000 - 3s - loss: 2.1579 - accuracy: 0.2012 - val_loss: 2.0880 - val_accuracy: 0.2182\n",
            "Epoch 4/200\n",
            "1000/1000 - 3s - loss: 2.0552 - accuracy: 0.2351 - val_loss: 2.0219 - val_accuracy: 0.2534\n",
            "Epoch 5/200\n",
            "1000/1000 - 3s - loss: 2.0075 - accuracy: 0.2589 - val_loss: 1.9851 - val_accuracy: 0.2703\n",
            "Epoch 6/200\n",
            "1000/1000 - 3s - loss: 1.9669 - accuracy: 0.2749 - val_loss: 1.9448 - val_accuracy: 0.2948\n",
            "Epoch 7/200\n",
            "1000/1000 - 3s - loss: 1.9233 - accuracy: 0.2952 - val_loss: 1.8923 - val_accuracy: 0.3098\n",
            "Epoch 8/200\n",
            "1000/1000 - 3s - loss: 1.8763 - accuracy: 0.3149 - val_loss: 1.8511 - val_accuracy: 0.3337\n",
            "Epoch 9/200\n",
            "1000/1000 - 3s - loss: 1.8262 - accuracy: 0.3359 - val_loss: 1.8393 - val_accuracy: 0.3373\n",
            "Epoch 10/200\n",
            "1000/1000 - 3s - loss: 1.7802 - accuracy: 0.3539 - val_loss: 1.7451 - val_accuracy: 0.3669\n",
            "Epoch 11/200\n",
            "1000/1000 - 3s - loss: 1.7366 - accuracy: 0.3694 - val_loss: 1.7372 - val_accuracy: 0.3757\n",
            "Epoch 12/200\n",
            "1000/1000 - 3s - loss: 1.6984 - accuracy: 0.3805 - val_loss: 1.6702 - val_accuracy: 0.3943\n",
            "Epoch 13/200\n",
            "1000/1000 - 3s - loss: 1.6701 - accuracy: 0.3908 - val_loss: 1.6463 - val_accuracy: 0.4023\n",
            "Epoch 14/200\n",
            "1000/1000 - 3s - loss: 1.6430 - accuracy: 0.4014 - val_loss: 1.6396 - val_accuracy: 0.4036\n",
            "Epoch 15/200\n",
            "1000/1000 - 3s - loss: 1.6182 - accuracy: 0.4077 - val_loss: 1.6057 - val_accuracy: 0.4177\n",
            "Epoch 16/200\n",
            "1000/1000 - 3s - loss: 1.5970 - accuracy: 0.4176 - val_loss: 1.5929 - val_accuracy: 0.4223\n",
            "Epoch 17/200\n",
            "1000/1000 - 3s - loss: 1.5799 - accuracy: 0.4219 - val_loss: 1.5891 - val_accuracy: 0.4184\n",
            "Epoch 18/200\n",
            "1000/1000 - 3s - loss: 1.5622 - accuracy: 0.4313 - val_loss: 1.5589 - val_accuracy: 0.4316\n",
            "Epoch 19/200\n",
            "1000/1000 - 3s - loss: 1.5477 - accuracy: 0.4353 - val_loss: 1.5508 - val_accuracy: 0.4384\n",
            "Epoch 20/200\n",
            "1000/1000 - 3s - loss: 1.5334 - accuracy: 0.4416 - val_loss: 1.5224 - val_accuracy: 0.4490\n",
            "Epoch 21/200\n",
            "1000/1000 - 3s - loss: 1.5188 - accuracy: 0.4462 - val_loss: 1.5117 - val_accuracy: 0.4518\n",
            "Epoch 22/200\n",
            "1000/1000 - 3s - loss: 1.5068 - accuracy: 0.4514 - val_loss: 1.5036 - val_accuracy: 0.4574\n",
            "Epoch 23/200\n",
            "1000/1000 - 3s - loss: 1.4949 - accuracy: 0.4549 - val_loss: 1.4814 - val_accuracy: 0.4626\n",
            "Epoch 24/200\n",
            "1000/1000 - 3s - loss: 1.4844 - accuracy: 0.4605 - val_loss: 1.4788 - val_accuracy: 0.4669\n",
            "Epoch 25/200\n",
            "1000/1000 - 3s - loss: 1.4707 - accuracy: 0.4668 - val_loss: 1.4875 - val_accuracy: 0.4583\n",
            "Epoch 26/200\n",
            "1000/1000 - 3s - loss: 1.4620 - accuracy: 0.4708 - val_loss: 1.4504 - val_accuracy: 0.4768\n",
            "Epoch 27/200\n",
            "1000/1000 - 3s - loss: 1.4499 - accuracy: 0.4748 - val_loss: 1.4507 - val_accuracy: 0.4783\n",
            "Epoch 28/200\n",
            "1000/1000 - 3s - loss: 1.4386 - accuracy: 0.4791 - val_loss: 1.4417 - val_accuracy: 0.4749\n",
            "Epoch 29/200\n",
            "1000/1000 - 3s - loss: 1.4276 - accuracy: 0.4831 - val_loss: 1.4296 - val_accuracy: 0.4883\n",
            "Epoch 30/200\n",
            "1000/1000 - 3s - loss: 1.4164 - accuracy: 0.4880 - val_loss: 1.4460 - val_accuracy: 0.4785\n",
            "Epoch 31/200\n",
            "1000/1000 - 3s - loss: 1.4079 - accuracy: 0.4932 - val_loss: 1.4035 - val_accuracy: 0.4944\n",
            "Epoch 32/200\n",
            "1000/1000 - 3s - loss: 1.3982 - accuracy: 0.4958 - val_loss: 1.4167 - val_accuracy: 0.4883\n",
            "Epoch 33/200\n",
            "1000/1000 - 3s - loss: 1.3870 - accuracy: 0.5006 - val_loss: 1.3973 - val_accuracy: 0.4951\n",
            "Epoch 34/200\n",
            "1000/1000 - 3s - loss: 1.3779 - accuracy: 0.5047 - val_loss: 1.4149 - val_accuracy: 0.4951\n",
            "Epoch 35/200\n",
            "1000/1000 - 3s - loss: 1.3674 - accuracy: 0.5093 - val_loss: 1.3708 - val_accuracy: 0.5093\n",
            "Epoch 36/200\n",
            "1000/1000 - 3s - loss: 1.3578 - accuracy: 0.5139 - val_loss: 1.3606 - val_accuracy: 0.5074\n",
            "Epoch 37/200\n",
            "1000/1000 - 3s - loss: 1.3490 - accuracy: 0.5151 - val_loss: 1.3491 - val_accuracy: 0.5149\n",
            "Epoch 38/200\n",
            "1000/1000 - 3s - loss: 1.3396 - accuracy: 0.5200 - val_loss: 1.3369 - val_accuracy: 0.5188\n",
            "Epoch 39/200\n",
            "1000/1000 - 3s - loss: 1.3315 - accuracy: 0.5215 - val_loss: 1.3473 - val_accuracy: 0.5191\n",
            "Epoch 40/200\n",
            "1000/1000 - 3s - loss: 1.3212 - accuracy: 0.5276 - val_loss: 1.3269 - val_accuracy: 0.5215\n",
            "Epoch 41/200\n",
            "1000/1000 - 3s - loss: 1.3132 - accuracy: 0.5300 - val_loss: 1.3365 - val_accuracy: 0.5202\n",
            "Epoch 42/200\n",
            "1000/1000 - 3s - loss: 1.3047 - accuracy: 0.5361 - val_loss: 1.3098 - val_accuracy: 0.5307\n",
            "Epoch 43/200\n",
            "1000/1000 - 3s - loss: 1.2977 - accuracy: 0.5372 - val_loss: 1.3129 - val_accuracy: 0.5294\n",
            "Epoch 44/200\n",
            "1000/1000 - 3s - loss: 1.2899 - accuracy: 0.5407 - val_loss: 1.3013 - val_accuracy: 0.5314\n",
            "Epoch 45/200\n",
            "1000/1000 - 3s - loss: 1.2818 - accuracy: 0.5428 - val_loss: 1.2914 - val_accuracy: 0.5356\n",
            "Epoch 46/200\n",
            "1000/1000 - 3s - loss: 1.2716 - accuracy: 0.5477 - val_loss: 1.2889 - val_accuracy: 0.5383\n",
            "Epoch 47/200\n",
            "1000/1000 - 3s - loss: 1.2655 - accuracy: 0.5504 - val_loss: 1.2754 - val_accuracy: 0.5466\n",
            "Epoch 48/200\n",
            "1000/1000 - 3s - loss: 1.2585 - accuracy: 0.5533 - val_loss: 1.2706 - val_accuracy: 0.5495\n",
            "Epoch 49/200\n",
            "1000/1000 - 3s - loss: 1.2499 - accuracy: 0.5556 - val_loss: 1.2585 - val_accuracy: 0.5533\n",
            "Epoch 50/200\n",
            "1000/1000 - 3s - loss: 1.2421 - accuracy: 0.5588 - val_loss: 1.2518 - val_accuracy: 0.5562\n",
            "Epoch 51/200\n",
            "1000/1000 - 3s - loss: 1.2376 - accuracy: 0.5622 - val_loss: 1.2542 - val_accuracy: 0.5522\n",
            "Epoch 52/200\n",
            "1000/1000 - 3s - loss: 1.2287 - accuracy: 0.5652 - val_loss: 1.2471 - val_accuracy: 0.5528\n",
            "Epoch 53/200\n",
            "1000/1000 - 3s - loss: 1.2226 - accuracy: 0.5686 - val_loss: 1.2341 - val_accuracy: 0.5600\n",
            "Epoch 54/200\n",
            "1000/1000 - 3s - loss: 1.2162 - accuracy: 0.5688 - val_loss: 1.2646 - val_accuracy: 0.5570\n",
            "Epoch 55/200\n",
            "1000/1000 - 3s - loss: 1.2092 - accuracy: 0.5714 - val_loss: 1.2218 - val_accuracy: 0.5633\n",
            "Epoch 56/200\n",
            "1000/1000 - 3s - loss: 1.2037 - accuracy: 0.5734 - val_loss: 1.2160 - val_accuracy: 0.5679\n",
            "Epoch 57/200\n",
            "1000/1000 - 3s - loss: 1.1972 - accuracy: 0.5780 - val_loss: 1.2344 - val_accuracy: 0.5685\n",
            "Epoch 58/200\n",
            "1000/1000 - 3s - loss: 1.1902 - accuracy: 0.5808 - val_loss: 1.2315 - val_accuracy: 0.5583\n",
            "Epoch 59/200\n",
            "1000/1000 - 3s - loss: 1.1854 - accuracy: 0.5810 - val_loss: 1.2047 - val_accuracy: 0.5746\n",
            "Epoch 60/200\n",
            "1000/1000 - 3s - loss: 1.1790 - accuracy: 0.5832 - val_loss: 1.1949 - val_accuracy: 0.5776\n",
            "Epoch 61/200\n",
            "1000/1000 - 3s - loss: 1.1748 - accuracy: 0.5849 - val_loss: 1.1993 - val_accuracy: 0.5716\n",
            "Epoch 62/200\n",
            "1000/1000 - 3s - loss: 1.1681 - accuracy: 0.5884 - val_loss: 1.1979 - val_accuracy: 0.5771\n",
            "Epoch 63/200\n",
            "1000/1000 - 3s - loss: 1.1628 - accuracy: 0.5905 - val_loss: 1.1810 - val_accuracy: 0.5826\n",
            "Epoch 64/200\n",
            "1000/1000 - 3s - loss: 1.1563 - accuracy: 0.5905 - val_loss: 1.1748 - val_accuracy: 0.5842\n",
            "Epoch 65/200\n",
            "1000/1000 - 3s - loss: 1.1518 - accuracy: 0.5943 - val_loss: 1.1821 - val_accuracy: 0.5846\n",
            "Epoch 66/200\n",
            "1000/1000 - 3s - loss: 1.1462 - accuracy: 0.5957 - val_loss: 1.1701 - val_accuracy: 0.5860\n",
            "Epoch 67/200\n",
            "1000/1000 - 3s - loss: 1.1425 - accuracy: 0.5964 - val_loss: 1.1642 - val_accuracy: 0.5913\n",
            "Epoch 68/200\n",
            "1000/1000 - 3s - loss: 1.1364 - accuracy: 0.5978 - val_loss: 1.1770 - val_accuracy: 0.5856\n",
            "Epoch 69/200\n",
            "1000/1000 - 3s - loss: 1.1322 - accuracy: 0.6000 - val_loss: 1.1590 - val_accuracy: 0.5945\n",
            "Epoch 70/200\n",
            "1000/1000 - 3s - loss: 1.1269 - accuracy: 0.6031 - val_loss: 1.1553 - val_accuracy: 0.5922\n",
            "Epoch 71/200\n",
            "1000/1000 - 3s - loss: 1.1225 - accuracy: 0.6049 - val_loss: 1.1575 - val_accuracy: 0.5948\n",
            "Epoch 72/200\n",
            "1000/1000 - 3s - loss: 1.1196 - accuracy: 0.6039 - val_loss: 1.1602 - val_accuracy: 0.5995\n",
            "Epoch 73/200\n",
            "1000/1000 - 3s - loss: 1.1140 - accuracy: 0.6076 - val_loss: 1.1413 - val_accuracy: 0.5999\n",
            "Epoch 74/200\n",
            "1000/1000 - 3s - loss: 1.1101 - accuracy: 0.6088 - val_loss: 1.1705 - val_accuracy: 0.5903\n",
            "Epoch 75/200\n",
            "1000/1000 - 3s - loss: 1.1036 - accuracy: 0.6101 - val_loss: 1.1491 - val_accuracy: 0.5930\n",
            "Epoch 76/200\n",
            "1000/1000 - 3s - loss: 1.1005 - accuracy: 0.6127 - val_loss: 1.1368 - val_accuracy: 0.6034\n",
            "Epoch 77/200\n",
            "1000/1000 - 3s - loss: 1.0953 - accuracy: 0.6152 - val_loss: 1.1358 - val_accuracy: 0.6040\n",
            "Epoch 78/200\n",
            "1000/1000 - 3s - loss: 1.0924 - accuracy: 0.6148 - val_loss: 1.1383 - val_accuracy: 0.6007\n",
            "Epoch 79/200\n",
            "1000/1000 - 3s - loss: 1.0877 - accuracy: 0.6165 - val_loss: 1.1211 - val_accuracy: 0.6059\n",
            "Epoch 80/200\n",
            "1000/1000 - 3s - loss: 1.0825 - accuracy: 0.6190 - val_loss: 1.1374 - val_accuracy: 0.5950\n",
            "Epoch 81/200\n",
            "1000/1000 - 3s - loss: 1.0798 - accuracy: 0.6198 - val_loss: 1.1255 - val_accuracy: 0.6072\n",
            "Epoch 82/200\n",
            "1000/1000 - 3s - loss: 1.0755 - accuracy: 0.6217 - val_loss: 1.1165 - val_accuracy: 0.6086\n",
            "Epoch 83/200\n",
            "1000/1000 - 3s - loss: 1.0709 - accuracy: 0.6235 - val_loss: 1.1107 - val_accuracy: 0.6105\n",
            "Epoch 84/200\n",
            "1000/1000 - 3s - loss: 1.0672 - accuracy: 0.6248 - val_loss: 1.1387 - val_accuracy: 0.6022\n",
            "Epoch 85/200\n",
            "1000/1000 - 3s - loss: 1.0619 - accuracy: 0.6252 - val_loss: 1.1081 - val_accuracy: 0.6141\n",
            "Epoch 86/200\n",
            "1000/1000 - 3s - loss: 1.0594 - accuracy: 0.6280 - val_loss: 1.1325 - val_accuracy: 0.6025\n",
            "Epoch 87/200\n",
            "1000/1000 - 3s - loss: 1.0561 - accuracy: 0.6289 - val_loss: 1.1165 - val_accuracy: 0.6079\n",
            "Epoch 88/200\n",
            "1000/1000 - 3s - loss: 1.0510 - accuracy: 0.6292 - val_loss: 1.1061 - val_accuracy: 0.6139\n",
            "Epoch 89/200\n",
            "1000/1000 - 3s - loss: 1.0470 - accuracy: 0.6319 - val_loss: 1.0984 - val_accuracy: 0.6157\n",
            "Epoch 90/200\n",
            "1000/1000 - 3s - loss: 1.0437 - accuracy: 0.6341 - val_loss: 1.1093 - val_accuracy: 0.6184\n",
            "Epoch 91/200\n",
            "1000/1000 - 3s - loss: 1.0404 - accuracy: 0.6339 - val_loss: 1.1235 - val_accuracy: 0.6109\n",
            "Epoch 92/200\n",
            "1000/1000 - 3s - loss: 1.0370 - accuracy: 0.6348 - val_loss: 1.0812 - val_accuracy: 0.6222\n",
            "Epoch 93/200\n",
            "1000/1000 - 3s - loss: 1.0321 - accuracy: 0.6389 - val_loss: 1.0843 - val_accuracy: 0.6202\n",
            "Epoch 94/200\n",
            "1000/1000 - 3s - loss: 1.0295 - accuracy: 0.6379 - val_loss: 1.0840 - val_accuracy: 0.6234\n",
            "Epoch 95/200\n",
            "1000/1000 - 3s - loss: 1.0260 - accuracy: 0.6412 - val_loss: 1.0914 - val_accuracy: 0.6167\n",
            "Epoch 96/200\n",
            "1000/1000 - 3s - loss: 1.0220 - accuracy: 0.6405 - val_loss: 1.0720 - val_accuracy: 0.6242\n",
            "Epoch 97/200\n",
            "1000/1000 - 3s - loss: 1.0173 - accuracy: 0.6437 - val_loss: 1.0952 - val_accuracy: 0.6194\n",
            "Epoch 98/200\n",
            "1000/1000 - 3s - loss: 1.0143 - accuracy: 0.6441 - val_loss: 1.0749 - val_accuracy: 0.6250\n",
            "Epoch 99/200\n",
            "1000/1000 - 3s - loss: 1.0092 - accuracy: 0.6473 - val_loss: 1.0696 - val_accuracy: 0.6287\n",
            "Epoch 100/200\n",
            "1000/1000 - 3s - loss: 1.0075 - accuracy: 0.6479 - val_loss: 1.0586 - val_accuracy: 0.6314\n",
            "Epoch 101/200\n",
            "1000/1000 - 3s - loss: 1.0035 - accuracy: 0.6482 - val_loss: 1.0910 - val_accuracy: 0.6227\n",
            "Epoch 102/200\n",
            "1000/1000 - 3s - loss: 1.0011 - accuracy: 0.6486 - val_loss: 1.0623 - val_accuracy: 0.6312\n",
            "Epoch 103/200\n",
            "1000/1000 - 3s - loss: 0.9955 - accuracy: 0.6519 - val_loss: 1.0705 - val_accuracy: 0.6280\n",
            "Epoch 104/200\n",
            "1000/1000 - 3s - loss: 0.9933 - accuracy: 0.6521 - val_loss: 1.0548 - val_accuracy: 0.6319\n",
            "Epoch 105/200\n",
            "1000/1000 - 3s - loss: 0.9889 - accuracy: 0.6554 - val_loss: 1.0824 - val_accuracy: 0.6203\n",
            "Epoch 106/200\n",
            "1000/1000 - 3s - loss: 0.9865 - accuracy: 0.6561 - val_loss: 1.0553 - val_accuracy: 0.6378\n",
            "Epoch 107/200\n",
            "1000/1000 - 3s - loss: 0.9830 - accuracy: 0.6557 - val_loss: 1.0702 - val_accuracy: 0.6269\n",
            "Epoch 108/200\n",
            "1000/1000 - 3s - loss: 0.9807 - accuracy: 0.6560 - val_loss: 1.0445 - val_accuracy: 0.6369\n",
            "Epoch 109/200\n",
            "1000/1000 - 3s - loss: 0.9759 - accuracy: 0.6593 - val_loss: 1.0688 - val_accuracy: 0.6283\n",
            "Epoch 110/200\n",
            "1000/1000 - 3s - loss: 0.9736 - accuracy: 0.6583 - val_loss: 1.0501 - val_accuracy: 0.6354\n",
            "Epoch 111/200\n",
            "1000/1000 - 3s - loss: 0.9697 - accuracy: 0.6607 - val_loss: 1.0440 - val_accuracy: 0.6398\n",
            "Epoch 112/200\n",
            "1000/1000 - 3s - loss: 0.9666 - accuracy: 0.6610 - val_loss: 1.0391 - val_accuracy: 0.6362\n",
            "Epoch 113/200\n",
            "1000/1000 - 3s - loss: 0.9631 - accuracy: 0.6622 - val_loss: 1.0336 - val_accuracy: 0.6420\n",
            "Epoch 114/200\n",
            "1000/1000 - 3s - loss: 0.9601 - accuracy: 0.6638 - val_loss: 1.0371 - val_accuracy: 0.6389\n",
            "Epoch 115/200\n",
            "1000/1000 - 3s - loss: 0.9556 - accuracy: 0.6655 - val_loss: 1.0277 - val_accuracy: 0.6426\n",
            "Epoch 116/200\n",
            "1000/1000 - 3s - loss: 0.9543 - accuracy: 0.6654 - val_loss: 1.0260 - val_accuracy: 0.6430\n",
            "Epoch 117/200\n",
            "1000/1000 - 3s - loss: 0.9511 - accuracy: 0.6666 - val_loss: 1.0240 - val_accuracy: 0.6470\n",
            "Epoch 118/200\n",
            "1000/1000 - 3s - loss: 0.9491 - accuracy: 0.6698 - val_loss: 1.0500 - val_accuracy: 0.6342\n",
            "Epoch 119/200\n",
            "1000/1000 - 3s - loss: 0.9455 - accuracy: 0.6689 - val_loss: 1.0154 - val_accuracy: 0.6482\n",
            "Epoch 120/200\n",
            "1000/1000 - 3s - loss: 0.9420 - accuracy: 0.6686 - val_loss: 1.0301 - val_accuracy: 0.6443\n",
            "Epoch 121/200\n",
            "1000/1000 - 3s - loss: 0.9391 - accuracy: 0.6714 - val_loss: 1.0256 - val_accuracy: 0.6418\n",
            "Epoch 122/200\n",
            "1000/1000 - 3s - loss: 0.9354 - accuracy: 0.6731 - val_loss: 1.0344 - val_accuracy: 0.6413\n",
            "Epoch 123/200\n",
            "1000/1000 - 3s - loss: 0.9322 - accuracy: 0.6745 - val_loss: 1.0080 - val_accuracy: 0.6494\n",
            "Epoch 124/200\n",
            "1000/1000 - 3s - loss: 0.9292 - accuracy: 0.6737 - val_loss: 1.0110 - val_accuracy: 0.6495\n",
            "Epoch 125/200\n",
            "1000/1000 - 3s - loss: 0.9272 - accuracy: 0.6765 - val_loss: 1.0112 - val_accuracy: 0.6492\n",
            "Epoch 126/200\n",
            "1000/1000 - 3s - loss: 0.9229 - accuracy: 0.6770 - val_loss: 1.0180 - val_accuracy: 0.6513\n",
            "Epoch 127/200\n",
            "1000/1000 - 3s - loss: 0.9205 - accuracy: 0.6775 - val_loss: 1.0419 - val_accuracy: 0.6351\n",
            "Epoch 128/200\n",
            "1000/1000 - 3s - loss: 0.9182 - accuracy: 0.6806 - val_loss: 1.0196 - val_accuracy: 0.6459\n",
            "Epoch 129/200\n",
            "1000/1000 - 3s - loss: 0.9158 - accuracy: 0.6806 - val_loss: 1.0047 - val_accuracy: 0.6511\n",
            "Epoch 130/200\n",
            "1000/1000 - 3s - loss: 0.9125 - accuracy: 0.6804 - val_loss: 0.9962 - val_accuracy: 0.6540\n",
            "Epoch 131/200\n",
            "1000/1000 - 3s - loss: 0.9089 - accuracy: 0.6835 - val_loss: 1.0203 - val_accuracy: 0.6439\n",
            "Epoch 132/200\n",
            "1000/1000 - 3s - loss: 0.9073 - accuracy: 0.6821 - val_loss: 0.9921 - val_accuracy: 0.6560\n",
            "Epoch 133/200\n",
            "1000/1000 - 3s - loss: 0.9036 - accuracy: 0.6848 - val_loss: 1.0353 - val_accuracy: 0.6419\n",
            "Epoch 134/200\n",
            "1000/1000 - 3s - loss: 0.9018 - accuracy: 0.6840 - val_loss: 1.0310 - val_accuracy: 0.6419\n",
            "Epoch 135/200\n",
            "1000/1000 - 3s - loss: 0.9007 - accuracy: 0.6847 - val_loss: 1.0144 - val_accuracy: 0.6476\n",
            "Epoch 136/200\n",
            "1000/1000 - 3s - loss: 0.8958 - accuracy: 0.6877 - val_loss: 1.0131 - val_accuracy: 0.6492\n",
            "Epoch 137/200\n",
            "1000/1000 - 3s - loss: 0.8941 - accuracy: 0.6869 - val_loss: 1.0163 - val_accuracy: 0.6511\n",
            "Epoch 138/200\n",
            "1000/1000 - 3s - loss: 0.8919 - accuracy: 0.6879 - val_loss: 1.0370 - val_accuracy: 0.6431\n",
            "Epoch 139/200\n",
            "1000/1000 - 3s - loss: 0.8897 - accuracy: 0.6899 - val_loss: 1.0152 - val_accuracy: 0.6506\n",
            "Epoch 140/200\n",
            "1000/1000 - 3s - loss: 0.8866 - accuracy: 0.6902 - val_loss: 0.9842 - val_accuracy: 0.6585\n",
            "Epoch 141/200\n",
            "1000/1000 - 3s - loss: 0.8834 - accuracy: 0.6908 - val_loss: 0.9982 - val_accuracy: 0.6555\n",
            "Epoch 142/200\n",
            "1000/1000 - 3s - loss: 0.8802 - accuracy: 0.6930 - val_loss: 0.9901 - val_accuracy: 0.6557\n",
            "Epoch 143/200\n",
            "1000/1000 - 3s - loss: 0.8784 - accuracy: 0.6926 - val_loss: 0.9782 - val_accuracy: 0.6610\n",
            "Epoch 144/200\n",
            "1000/1000 - 3s - loss: 0.8764 - accuracy: 0.6957 - val_loss: 0.9881 - val_accuracy: 0.6586\n",
            "Epoch 145/200\n",
            "1000/1000 - 3s - loss: 0.8737 - accuracy: 0.6939 - val_loss: 0.9946 - val_accuracy: 0.6541\n",
            "Epoch 146/200\n",
            "1000/1000 - 3s - loss: 0.8711 - accuracy: 0.6957 - val_loss: 0.9792 - val_accuracy: 0.6585\n",
            "Epoch 147/200\n",
            "1000/1000 - 3s - loss: 0.8700 - accuracy: 0.6968 - val_loss: 0.9967 - val_accuracy: 0.6505\n",
            "Epoch 148/200\n",
            "1000/1000 - 3s - loss: 0.8663 - accuracy: 0.6974 - val_loss: 0.9837 - val_accuracy: 0.6623\n",
            "Epoch 149/200\n",
            "1000/1000 - 3s - loss: 0.8638 - accuracy: 0.6980 - val_loss: 0.9750 - val_accuracy: 0.6648\n",
            "Epoch 150/200\n",
            "1000/1000 - 3s - loss: 0.8631 - accuracy: 0.6993 - val_loss: 0.9729 - val_accuracy: 0.6619\n",
            "Epoch 151/200\n",
            "1000/1000 - 3s - loss: 0.8600 - accuracy: 0.7002 - val_loss: 0.9714 - val_accuracy: 0.6637\n",
            "Epoch 152/200\n",
            "1000/1000 - 3s - loss: 0.8583 - accuracy: 0.6998 - val_loss: 0.9664 - val_accuracy: 0.6664\n",
            "Epoch 153/200\n",
            "1000/1000 - 3s - loss: 0.8558 - accuracy: 0.7013 - val_loss: 0.9872 - val_accuracy: 0.6586\n",
            "Epoch 154/200\n",
            "1000/1000 - 3s - loss: 0.8528 - accuracy: 0.7017 - val_loss: 0.9775 - val_accuracy: 0.6657\n",
            "Epoch 155/200\n",
            "1000/1000 - 3s - loss: 0.8511 - accuracy: 0.7038 - val_loss: 0.9753 - val_accuracy: 0.6652\n",
            "Epoch 156/200\n",
            "1000/1000 - 3s - loss: 0.8477 - accuracy: 0.7042 - val_loss: 0.9736 - val_accuracy: 0.6624\n",
            "Epoch 157/200\n",
            "1000/1000 - 3s - loss: 0.8456 - accuracy: 0.7049 - val_loss: 0.9631 - val_accuracy: 0.6647\n",
            "Epoch 158/200\n",
            "1000/1000 - 3s - loss: 0.8432 - accuracy: 0.7056 - val_loss: 0.9861 - val_accuracy: 0.6569\n",
            "Epoch 159/200\n",
            "1000/1000 - 3s - loss: 0.8431 - accuracy: 0.7059 - val_loss: 0.9664 - val_accuracy: 0.6669\n",
            "Epoch 160/200\n",
            "1000/1000 - 3s - loss: 0.8394 - accuracy: 0.7061 - val_loss: 0.9721 - val_accuracy: 0.6685\n",
            "Epoch 161/200\n",
            "1000/1000 - 3s - loss: 0.8386 - accuracy: 0.7077 - val_loss: 0.9690 - val_accuracy: 0.6647\n",
            "Epoch 162/200\n",
            "1000/1000 - 3s - loss: 0.8372 - accuracy: 0.7089 - val_loss: 0.9709 - val_accuracy: 0.6663\n",
            "Epoch 163/200\n",
            "1000/1000 - 3s - loss: 0.8330 - accuracy: 0.7107 - val_loss: 0.9663 - val_accuracy: 0.6666\n",
            "Epoch 164/200\n",
            "1000/1000 - 3s - loss: 0.8317 - accuracy: 0.7108 - val_loss: 1.0074 - val_accuracy: 0.6541\n",
            "Epoch 165/200\n",
            "1000/1000 - 3s - loss: 0.8294 - accuracy: 0.7096 - val_loss: 0.9680 - val_accuracy: 0.6678\n",
            "Epoch 166/200\n",
            "1000/1000 - 3s - loss: 0.8258 - accuracy: 0.7122 - val_loss: 0.9788 - val_accuracy: 0.6604\n",
            "Epoch 167/200\n",
            "1000/1000 - 3s - loss: 0.8240 - accuracy: 0.7108 - val_loss: 0.9532 - val_accuracy: 0.6730\n",
            "Epoch 168/200\n",
            "1000/1000 - 3s - loss: 0.8220 - accuracy: 0.7136 - val_loss: 0.9640 - val_accuracy: 0.6684\n",
            "Epoch 169/200\n",
            "1000/1000 - 3s - loss: 0.8214 - accuracy: 0.7139 - val_loss: 0.9536 - val_accuracy: 0.6720\n",
            "Epoch 170/200\n",
            "1000/1000 - 3s - loss: 0.8198 - accuracy: 0.7149 - val_loss: 0.9515 - val_accuracy: 0.6733\n",
            "Epoch 171/200\n",
            "1000/1000 - 3s - loss: 0.8175 - accuracy: 0.7150 - val_loss: 0.9560 - val_accuracy: 0.6677\n",
            "Epoch 172/200\n",
            "1000/1000 - 3s - loss: 0.8151 - accuracy: 0.7157 - val_loss: 0.9512 - val_accuracy: 0.6700\n",
            "Epoch 173/200\n",
            "1000/1000 - 3s - loss: 0.8128 - accuracy: 0.7140 - val_loss: 0.9696 - val_accuracy: 0.6709\n",
            "Epoch 174/200\n",
            "1000/1000 - 3s - loss: 0.8120 - accuracy: 0.7165 - val_loss: 0.9595 - val_accuracy: 0.6744\n",
            "Epoch 175/200\n",
            "1000/1000 - 3s - loss: 0.8106 - accuracy: 0.7173 - val_loss: 0.9524 - val_accuracy: 0.6702\n",
            "Epoch 176/200\n",
            "1000/1000 - 3s - loss: 0.8082 - accuracy: 0.7172 - val_loss: 0.9729 - val_accuracy: 0.6659\n",
            "Epoch 177/200\n",
            "1000/1000 - 3s - loss: 0.8051 - accuracy: 0.7180 - val_loss: 0.9863 - val_accuracy: 0.6618\n",
            "Epoch 178/200\n",
            "1000/1000 - 3s - loss: 0.8037 - accuracy: 0.7199 - val_loss: 0.9476 - val_accuracy: 0.6765\n",
            "Epoch 179/200\n",
            "1000/1000 - 3s - loss: 0.8029 - accuracy: 0.7204 - val_loss: 0.9447 - val_accuracy: 0.6752\n",
            "Epoch 180/200\n",
            "1000/1000 - 3s - loss: 0.7997 - accuracy: 0.7216 - val_loss: 0.9511 - val_accuracy: 0.6723\n",
            "Epoch 181/200\n",
            "1000/1000 - 3s - loss: 0.7972 - accuracy: 0.7221 - val_loss: 0.9436 - val_accuracy: 0.6750\n",
            "Epoch 182/200\n",
            "1000/1000 - 3s - loss: 0.7961 - accuracy: 0.7228 - val_loss: 0.9433 - val_accuracy: 0.6775\n",
            "Epoch 183/200\n",
            "1000/1000 - 3s - loss: 0.7944 - accuracy: 0.7228 - val_loss: 0.9532 - val_accuracy: 0.6729\n",
            "Epoch 184/200\n",
            "1000/1000 - 3s - loss: 0.7923 - accuracy: 0.7239 - val_loss: 0.9702 - val_accuracy: 0.6636\n",
            "Epoch 185/200\n",
            "1000/1000 - 3s - loss: 0.7906 - accuracy: 0.7232 - val_loss: 0.9740 - val_accuracy: 0.6630\n",
            "Epoch 186/200\n",
            "1000/1000 - 3s - loss: 0.7893 - accuracy: 0.7259 - val_loss: 0.9564 - val_accuracy: 0.6731\n",
            "Epoch 187/200\n",
            "1000/1000 - 3s - loss: 0.7863 - accuracy: 0.7252 - val_loss: 0.9461 - val_accuracy: 0.6745\n",
            "Epoch 188/200\n",
            "1000/1000 - 3s - loss: 0.7844 - accuracy: 0.7258 - val_loss: 0.9562 - val_accuracy: 0.6709\n",
            "Epoch 189/200\n",
            "1000/1000 - 3s - loss: 0.7826 - accuracy: 0.7277 - val_loss: 0.9443 - val_accuracy: 0.6739\n",
            "Epoch 190/200\n",
            "1000/1000 - 3s - loss: 0.7822 - accuracy: 0.7249 - val_loss: 0.9380 - val_accuracy: 0.6763\n",
            "Epoch 191/200\n",
            "1000/1000 - 3s - loss: 0.7790 - accuracy: 0.7271 - val_loss: 0.9441 - val_accuracy: 0.6770\n",
            "Epoch 192/200\n",
            "1000/1000 - 3s - loss: 0.7782 - accuracy: 0.7285 - val_loss: 0.9460 - val_accuracy: 0.6732\n",
            "Epoch 193/200\n",
            "1000/1000 - 3s - loss: 0.7771 - accuracy: 0.7287 - val_loss: 0.9457 - val_accuracy: 0.6716\n",
            "Epoch 194/200\n",
            "1000/1000 - 3s - loss: 0.7741 - accuracy: 0.7302 - val_loss: 0.9441 - val_accuracy: 0.6764\n",
            "Epoch 195/200\n",
            "1000/1000 - 3s - loss: 0.7713 - accuracy: 0.7313 - val_loss: 0.9525 - val_accuracy: 0.6742\n",
            "Epoch 196/200\n",
            "1000/1000 - 3s - loss: 0.7711 - accuracy: 0.7310 - val_loss: 0.9361 - val_accuracy: 0.6819\n",
            "Epoch 197/200\n",
            "1000/1000 - 3s - loss: 0.7674 - accuracy: 0.7324 - val_loss: 0.9412 - val_accuracy: 0.6784\n",
            "Epoch 198/200\n",
            "1000/1000 - 3s - loss: 0.7674 - accuracy: 0.7323 - val_loss: 0.9441 - val_accuracy: 0.6754\n",
            "Epoch 199/200\n",
            "1000/1000 - 3s - loss: 0.7652 - accuracy: 0.7327 - val_loss: 0.9544 - val_accuracy: 0.6715\n",
            "Epoch 200/200\n",
            "1000/1000 - 3s - loss: 0.7636 - accuracy: 0.7323 - val_loss: 0.9457 - val_accuracy: 0.6723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYp_Elw3iQ9U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}